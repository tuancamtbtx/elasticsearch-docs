{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lab03-WebCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJktAwbCOyod"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import string\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HFWqw1VOrEe"
      },
      "source": [
        "def get_url_in_response(text):\n",
        "    urls = []\n",
        "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text)\n",
        "    return urls\n",
        "\n",
        "def get_urls(url):\n",
        "    urls = []\n",
        "    try:\n",
        "        r = requests.get(url, verify=False, timeout=2)\n",
        "        # TODO\n",
        "        # Lấy các url nằm trong trang web của url này, lưu lại vào biến urls\n",
        "        urls = get_url_in_response(r.text)\n",
        "        return urls\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def insert_url_into_arr(urls, new_urls, limit):\n",
        "    count = 0;\n",
        "    for url in new_urls:\n",
        "        if count == limit:\n",
        "            break\n",
        "        if url not in urls:\n",
        "            urls.append(url)\n",
        "        count += 1\n",
        "    return urls\n",
        "\n",
        "def get_urls_recursive(start_url, limit):\n",
        "    urls = [start_url]\n",
        "    tmp_list = []\n",
        "    for url in urls:\n",
        "        if url not in tmp_list:\n",
        "            tmp_list.append(url)\n",
        "            new_urls = get_urls(url)\n",
        "            if len(new_urls) > 0:\n",
        "                insert_url_into_arr(urls,new_urls,limit )\n",
        "            print(len(urls))\n",
        "        if len(urls) > 200:\n",
        "            return urls[0:200]\n",
        "    return urls\n",
        "url_list = get_urls_recursive('https://www.hcmus.edu.vn', 200)\n",
        "# print(url_list)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n",
            "92\n",
            "98\n",
            "98\n",
            "An exception occurred\n",
            "98\n",
            "105\n",
            "108\n",
            "114\n",
            "118\n",
            "121\n",
            "125\n",
            "An exception occurred\n",
            "125\n",
            "125\n",
            "134\n",
            "156\n",
            "164\n",
            "176\n",
            "182\n",
            "185\n",
            "186\n",
            "186\n",
            "187\n",
            "304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB5nIZAhQr7-"
      },
      "source": [
        "def text_filter(element):\n",
        "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
        "        return False\n",
        "    elif isinstance(element, Comment):\n",
        "        '''Opinion mining?'''\n",
        "        return False\n",
        "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
        "        '''space, return, endline'''\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def wordList(url):\n",
        "    r =  None\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "    if r == None:\n",
        "        return \n",
        "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "    text = soup.findAll(text=True)\n",
        "    filtered_text = list(filter(text_filter, text)) # list của các chuỗi\n",
        "    word_list = []\n",
        "    # print(text)\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    for i in range(len(filtered_text)):\n",
        "        text_trans = filtered_text[i].translate(translator).strip()\n",
        "        text_split = text_trans.split(\" \")\n",
        "        for j in range(len(text_split)):\n",
        "            if len(text_split[j]) != 0:\n",
        "                word_list.append(text_split[j])\n",
        "    return ' '.join([str(elem) for elem in word_list])\n",
        "\n",
        "def read_url(url, url_idx, data):\n",
        "    word_list = wordList(url)\n",
        "    return word_list"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrPuiiDhQfrJ",
        "tags": []
      },
      "source": [
        "data = {}\n",
        "for url_index, url in enumerate(url_list[:1], 1):\n",
        "    read_url(url, url_index, data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'url_list' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-64fe20d10f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0murl_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mread_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'url_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open('data.json', 'w') as fp:\n",
        "    json.dump(rs, fp,ensure_ascii=False)"
      ]
    }
  ]
}