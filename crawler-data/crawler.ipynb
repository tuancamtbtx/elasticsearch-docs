{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lab03-WebCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJktAwbCOyod"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import string\n",
        "import pickle"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HFWqw1VOrEe"
      },
      "source": [
        "def get_url_in_response(text):\n",
        "    urls = []\n",
        "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text)\n",
        "    return urls\n",
        "\n",
        "def get_urls(url):\n",
        "    urls = []\n",
        "    try:\n",
        "        r = requests.get(url, verify=False, timeout=2)\n",
        "        # TODO\n",
        "        # Lấy các url nằm trong trang web của url này, lưu lại vào biến urls\n",
        "        urls = get_url_in_response(r.text)\n",
        "        return urls\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def insert_url_into_arr(urls, new_urls, limit):\n",
        "    count = 0;\n",
        "    for url in new_urls:\n",
        "        if count == limit:\n",
        "            break\n",
        "        if url not in urls:\n",
        "            urls.append(url)\n",
        "        count += 1\n",
        "    return urls\n",
        "\n",
        "def get_urls_recursive(start_url, limit):\n",
        "    urls = [start_url]\n",
        "    tmp_list = []\n",
        "    for url in urls:\n",
        "        if url not in tmp_list:\n",
        "            tmp_list.append(url)\n",
        "            new_urls = get_urls(url)\n",
        "            if len(new_urls) > 0:\n",
        "                insert_url_into_arr(urls,new_urls,limit )\n",
        "            print(len(urls))\n",
        "        if len(urls) > 200:\n",
        "            return urls[0:200]\n",
        "    return urls\n",
        "url_list = get_urls_recursive('https://www.hcmus.edu.vn', 200)\n",
        "# print(url_list)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n",
            "36\n",
            "36\n",
            "37\n",
            "48\n",
            "58\n",
            "61\n",
            "69\n",
            "73\n",
            "79\n",
            "83\n",
            "87\n",
            "90\n",
            "91\n",
            "91\n",
            "101\n",
            "113\n",
            "An exception occurred\n",
            "113\n",
            "114\n",
            "119\n",
            "120\n",
            "120\n",
            "141\n",
            "141\n",
            "146\n",
            "154\n",
            "174\n",
            "194\n",
            "212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB5nIZAhQr7-"
      },
      "source": [
        "def text_filter(element):\n",
        "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
        "        return False\n",
        "    elif isinstance(element, Comment):\n",
        "        '''Opinion mining?'''\n",
        "        return False\n",
        "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
        "        '''space, return, endline'''\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def wordList(url):\n",
        "    r =  None\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "    if r == None:\n",
        "        return \n",
        "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "    text = soup.findAll(text=True)\n",
        "    filtered_text = list(filter(text_filter, text)) # list của các chuỗi\n",
        "    word_list = []\n",
        "    # print(text)\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    for i in range(len(filtered_text)):\n",
        "        text_trans = filtered_text[i].translate(translator).strip()\n",
        "        text_split = text_trans.split(\" \")\n",
        "        for j in range(len(text_split)):\n",
        "            if len(text_split[j]) != 0:\n",
        "                word_list.append(text_split[j])\n",
        "    return ' '.join([str(elem) for elem in word_list])\n",
        "\n",
        "def read_url(url, url_idx, data):\n",
        "    content = wordList(url)\n",
        "    out_dict = dict()\n",
        "    out_dict[\"url\"] = url\n",
        "    out_dict[\"content\"] = content\n",
        "    return out_dict\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrPuiiDhQfrJ",
        "tags": []
      },
      "source": [
        "data = {}\n",
        "rs = []\n",
        "for url_index, url in enumerate(url_list, 1):\n",
        "    res = read_url(url, url_index, data)\n",
        "    rs.append(res)\n",
        "    # print(res)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An exception occurred\n",
            "An exception occurred\n",
            "An exception occurred\n",
            "An exception occurred\n",
            "An exception occurred\n",
            "An exception occurred\n",
            "An exception occurred\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open('data.json', 'w') as fp:\n",
        "    json.dump(rs, fp,ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# crawler data for search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}